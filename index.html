<!DOCTYPE html>
<html>
<header>
    <title>Segment Anything Frontend Only Demo</title>
</header>

<body>
    <h1>Segment Anything in the Browser</h1>
    <div>
        <input title="Image from File" type="file" id="file-in" name="file-in">
    </div>
    <div style="display: none;">
        <img id="original-image" src="#" />
    </div>
    <h3>Status</h3>
    <span id="status">No image uploaded</span>
    <h3>Resized Image and Mask</h3>
    <canvas id="canvas"></canvas>
    <!-- import ONNXRuntime Web from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"> </script>
    <script>
        var canvas = document.getElementById('canvas');
        const dimension = 1024;
        var image_embeddings;
        var imageImageData

        async function handleClick(event) {
            const rect = canvas.getBoundingClientRect();
            const x = event.clientX - rect.left;
            const y = event.clientY - rect.top;
            
            console.log('Clicked position:', x, y);
            document.getElementById("status").textContent = `Clicked on (${x}, ${y}). Downloading the decoder model if needed and generating mask...`;

            let context = canvas.getContext('2d');
            context.clearRect(0, 0, canvas.width, canvas.height);
            canvas.width = imageImageData.width;
            canvas.height = imageImageData.height;
            context.putImageData(imageImageData, 0, 0);
            context.fillStyle = 'green';
            context.fillRect(x, y, 10, 10);

            const pointCoords = new ort.Tensor(new Float32Array([x, y, 0, 0]), [1, 2, 2]);
            const pointLabels = new ort.Tensor(new Float32Array([0, -1]), [1, 2]);
            // generate a (1, 1, 256, 256) tensor with all values set to 0
            const maskInput = new ort.Tensor(new Float32Array(256 * 256), [1, 1, 256, 256]);
            const hasMask = new ort.Tensor(new Float32Array([0]), [1,]);
            const origianlImageSize = new ort.Tensor(new Float32Array([684, 1024]), [2,]);

            const decodingSession = await ort.InferenceSession.create('./models/sam_vit_b_01ec64.decoder.onnx');
            console.log("Decoder session", decodingSession);
            // Print type of image_embeddings
            console.log("Type of image_embeddings:", image_embeddings.type);
            const decodingFeeds = {
                "image_embeddings": image_embeddings,
                "point_coords": pointCoords,
                "point_labels": pointLabels,
                "mask_input": maskInput,
                "has_mask_input": hasMask,
                "orig_im_size": origianlImageSize
            }
            console.log("generating mask...");
            start = Date.now();
            try {
                results = await decodingSession.run(decodingFeeds);
                console.log("Generated mask:", results);
                const mask = results.masks;
                const maskImageData = mask.toImageData();
                context.globalAlpha = 0.5;
                // convert image data to image bitmap
                let imageBitmap = await createImageBitmap(maskImageData);
                context.drawImage(imageBitmap, 0, 0);

            } catch (error) {
                console.log(`caught error: ${error}`)
            }
            end = Date.now();
            console.log(`generating masks took ${(end - start)/1000} seconds`);
            document.getElementById("status").textContent = `Mask generated. Click on the image to generate new mask.`;
        }

        async function handleImage(img) {
            let logMessages = []; // Array to hold log messages

            function sendLog(title, value) {
                // Convert non-string values to a JSON string for logging
                const stringValue = typeof value === 'string' ? value : JSON.stringify(value, null, 2);
                
                // Create the log entry
                const logEntry = `${title}:\n${stringValue}`;
                
                // Output the log entry to the console and push it to the logMessages array
                console.log(logEntry);
                logMessages.push('---');
                logMessages.push(logEntry); // Add the log entry to the log array
            }

            function downloadLogs() {
                const blob = new Blob(logMessages.map(message => message + '\n'), { type: 'text/plain' });
                const url = URL.createObjectURL(blob);
                const a = document.createElement('a');
                a.href = url;
                a.download = 'logs.txt';
                a.click();
                URL.revokeObjectURL(url);
            }
            try {
                sendLog('Processing started for image of size:', `${img.width, 'x', img.height}`);
                document.getElementById("status").textContent = `Uploaded image of size ${img.width}x${img.height}. Downloading the encoder model (~100 MB) if not cached and generating embedding. This will take a minute...`;
                
                const scaleX = dimension / img.width;
                const scaleY = dimension / img.height;
                
                const resizedTensor = await ort.Tensor.fromImage(img, { resizedWidth: 1024, resizedHeight: 684 });
                sendLog('Resized image tensor shape:', resizedTensor.dims);
                
                const resizeImage = resizedTensor.toImageData();
                let imageDataTensor = await ort.Tensor.fromImage(resizeImage);
                sendLog('Image data tensor created:', imageDataTensor.dims);
                
                imageImageData = imageDataTensor.toImageData();
                sendLog('Image data tensor shape after conversion:', imageDataTensor.dims);
                
                // Presenting the images on the DOM
                const canvas = document.getElementById('canvas');
                canvas.width = imageImageData.width;
                canvas.height = imageImageData.height;
                let context = canvas.getContext('2d');
                context.putImageData(imageImageData, 0, 0);
                
                let tf_tensor = tf.tensor(imageDataTensor.data, imageDataTensor.dims);
                sendLog('Tensorflow tensor shape:', tf_tensor.shape);
                
                tf_tensor = tf_tensor.reshape([3, 684, 1024]);
                sendLog('Tensorflow tensor reshaped:', tf_tensor.shape);
                
                tf_tensor = tf_tensor.transpose([1, 2, 0]).mul(255);
                sendLog('Tensorflow tensor transposed:', tf_tensor.shape);
                
                imageDataTensor = new ort.Tensor(tf_tensor.dataSync(), tf_tensor.shape);
                sendLog('ORT tensor shape after transpose and scale:', imageDataTensor.dims);
                
                // const session = await ort.InferenceSession.create('https://files.sunu.in/sam_vit_b_01ec64.encoder.preprocess.quant.onnx');
                const session = await ort.InferenceSession.create('./models/sam_vit_b_01ec64.encoder.preprocess.quant.onnx');
                sendLog("Encoder Session created:", session);
                
                const feeds = { "input_image": imageDataTensor };
                // sendLog("Feeds prepared for session run:", feeds);
                
                let start = Date.now();
                let results;
                try {
                    results = await session.run(feeds);
                    sendLog("Encoding result obtained:", results);
                    
                    image_embeddings = results.image_embeddings;
                    sendLog('Image embeddings shape:', image_embeddings.dims);
                    
                } catch (error) {
                    console.error('Error during encoding:', error);
                    document.getElementById("status").textContent = `Error: ${error}`;
                }
                
                let end = Date.now();
                let time_taken = (end - start) / 1000;
                console.log(`Embedding computation time: ${time_taken} seconds`);
                document.getElementById("status").textContent = `Embedding generated in ${time_taken} seconds. Click on the image to generate mask.`;
                downloadLogs(); 
                canvas.addEventListener('click', handleClick);
            } catch (error) {
                console.error('Caught error in handleImage:', error);
                downloadLogs(); 
            }
        }


        function loadImage(fileReader) {
            var img = document.getElementById("original-image");
            img.onload = () => handleImage(img);
            img.src = fileReader.result;
        }

        // use an async context to call onnxruntime functions.
        async function main() {
            var img = document.getElementById("original-image");
            document.getElementById("file-in").onchange = function (evt) {
                let target = evt.target || window.event.src, files = target.files;
                if (FileReader && files && files.length) {
                    var fileReader = new FileReader();
                    fileReader.onload = () => loadImage(fileReader);
                    fileReader.readAsDataURL(files[0]);
                }
            };
        }
        function sendLog(message) {
            // Send a POST request to the server with the log message
            fetch('/log', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({ message }),
            })
            .then(response => response.text())
            .then(data => console.log('Log saved:', data))
            .catch((error) => console.error('Error logging:', error));
        }


        main();
    </script>
</body>

</html>